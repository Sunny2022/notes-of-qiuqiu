# PCA

两个矩阵相乘的意义是将右边矩阵中的每一列向量 ai 变换到左边矩阵中以每一行行向量为基所表示的空间中去

## 1. 最大可分性
如果基的数量少于向量本身的维数，则可以达到降维的效果。

### 如何选择基才是最优的?

或者说，如果我们有一组 N 维向量，现在要将其降到 K 维（K 小于 N），那么我们应该如何选择 K 个基才能最大程度保留原有的信息？

##### 一种直观的看法是：
希望投影后的投影值尽可能分散，因为如果重叠就会有样本消失。
当然这个也可以从熵的角度进行理解，熵越大所含信息越多。



##### 结论

最大方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应的特征向量，次佳就是第二大特征值对应的特征向量，以此类推



## 2. 最近重构性

**转换为线型回归问题**：其目标是求解一个<u>线性函数</u>使得对应直线能够更好地拟合样本点集合**。**

这就使得我们的优化目标从**方差最大转化为平方误差最小**，因为映射距离越短，丢失的信息也会越小。

## 3. 求解步骤

总结一下 PCA 的算法步骤：

设有 m 条 n 维数据。

1. 将原始数据按列组成 n 行 m 列矩阵 X；
2. 将 X 的每一行进行零均值化，即减去这一行的均值；
3. 求出协方差矩阵 ![[公式]](https://www.zhihu.com/equation?tex=C%3D%5Cfrac%7B1%7D%7Bm%7DXX%5E%5Cmathsf%7BT%7D) ；
4. 求出协方差矩阵的特征值及对应的特征向量；
5. 将特征向量按对应特征值大小从上到下按行排列成矩阵，取前 k 行组成矩阵 P；
6. ![[公式]](https://www.zhihu.com/equation?tex=Y%3DPX) 即为降维到 k 维后的数据。



###### pca 的缺点：

离群点影响很大，降维后可以降低离群点的影响。 链接：

[用最直观的方式告诉你：什么是主成分分析PCA-哔哩哔哩]: https://b23.tv/a1LkDf3





